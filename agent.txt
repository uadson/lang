from decouple import config
from langchain.chat_models import init_chat_model
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch


class Settings:
    GOOGLE_API_KEY: str = config("GOOGLE_API_KEY")
    LANGSMITH_API_KEY: str = config("LANGSMITH_API_KEY")
    LANGSMITH_PROJECT: str = config("LANGSMITH_PROJECT")
    LANGSMITH_TRACING: bool = config("LANGSMITH_TRACING")
    LANGSMITH_ENDPOINT: str = config("LANGSMITH_ENDPOINT")
    LLM_MODEL: str = config("LLM_MODEL")
    TAVILY_API_KEY: str = config("TAVILY_API_KEY")
    LANGSMITH_PROJECT: str = config("LANGSMITH_PROJECT")


settings: Settings = Settings()

settings.LANGSMITH_PROJECT
settings.LANGSMITH_API_KEY
settings.LANGSMITH_TRACING
settings.LANGSMITH_ENDPOINT
settings.LANGSMITH_PROJECT
settings.GOOGLE_API_KEY
settings.TAVILY_API_KEY

model = init_chat_model(
    str(settings.LLM_MODEL),
    model_provider="google-genai",
)

# --- 1. Inicia modelo Gemini ---
llm = ChatGoogleGenerativeAI(
    model=settings.LLM_MODEL, google_api_key=settings.GOOGLE_API_KEY, temperature=0.7
)

# --- 2. Inicia o cliente tavily
tavily_client = TavilySearch(max_results=5, tavily_api_key=settings.TAVILY_API_KEY)


from config import tavily_client, llm
from typing import List, Literal, TypedDict
from langgraph.graph import StateGraph, END
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser


# --- 1. Tipo do estado ---
class ChatState(TypedDict):
    chat_history: List[HumanMessage | AIMessage]
    input: str


# --- 2. Prompt do Chat ---
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Voc√™ √© um assistente √∫til e amig√°vel."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)


# --- 3. Classificador ---
def classificator_node(state: ChatState) -> Literal["use_tavily", "normal_response"]:
    """
    Decide se a consulta do usu√°rio requer uma busca na web.
    """
    text = state["input"].lower()

    # L√≥gica aprimorada: se a pergunta for factual, sobre os eventos atuais ou exigir informa√ß√£o externa.
    # Adicionamos mais termos e consideramos a inten√ß√£o de buscar informa√ß√µes espec√≠ficas.
    keywords_for_tavily = [
        "√∫ltimas",
        "hoje",
        "agora",
        "quem √©",
        "o que √©",
        "quando",
        "not√≠cia",
        "atual",
        "informa√ß√µes sobre",
        "fatos sobre",
        "previs√£o do tempo",
        "onde fica",
        "not√≠cias de",
        "acontecendo em",
        "qual o",
        "como est√°",
        "elei√ß√µes",
        "mercado financeiro",
        "eventos de hoje",
    ]

    if any(keyword in text for keyword in keywords_for_tavily):
        print(
            f"DEBUG: Palavra-chave detectada: {', '.join([k for k in keywords_for_tavily if k in text])} -> Usando Tavily."
        )
        return "use_tavily"

    print("DEBUG: Nenhuma palavra-chave factual detectada -> Resposta normal.")
    return "normal_response"


# --- 4. N√≥ com Tavily ---
def use_tavily_node(state: ChatState) -> ChatState:
    """
    Executa uma busca na web usando Tavily e retorna a informa√ß√£o encontrada.
    """
    query = state["input"]
    print(f"üîé Buscando na web via Tavily para: '{query}'")

    try:
        # A API TavilySearchResults retorna uma lista de dicion√°rios com 'content'
        results = tavily_client.invoke({"query": query})
        print(f"Tipo: {type(results)}")
        # O resultado de TavilySearch pode ser uma lista de strings ou dicion√°rios.
        # Se for uma lista de dicion√°rios com 'content', voc√™ pode querer concatenar.
        # Caso contr√°rio, dependendo do que `tavily_client.invoke` retorna, pode ser necess√°rio ajustar.

        # Para fins deste exemplo, vamos assumir que o resultado √© uma string com a resposta ou um resumo.
        # Em um caso real, voc√™ pode precisar iterar sobre 'results' e formatar.
        # Garante que a resposta seja texto
        if isinstance(results, str):
            info = results
        elif isinstance(results, dict) and "results" in results:
            info = " ".join([r["content"] for r in results["results"]])
        else:
            info = str(results)

        if not info:
            info = "N√£o encontrei uma resposta confi√°vel na web."
            print("DEBUG: Tavily n√£o retornou informa√ß√µes √∫teis.")
        else:
            print(f"DEBUG: Informa√ß√£o encontrada por Tavily: {str(info)[:150]}...")
            # Imprime os primeiros 150 caracteres

    except Exception as e:
        info = f"Ocorreu um erro ao buscar no Tavily: {e}"
        print(f"ERRO: {info}")

    updated_history = state["chat_history"] + [
        HumanMessage(content=query),
        AIMessage(content=info),  # A resposta do Tavily √© a resposta do "bot" neste n√≥
    ]

    # O input original √© mantido para que, se o fluxo continuar, ele ainda esteja dispon√≠vel.
    # No entanto, como este n√≥ leva ao END, o 'input' n√£o ser√° usado novamente neste fluxo.
    return {"chat_history": updated_history, "input": query}


# --- 5. N√≥ normal com LLM ---
def normal_response_node(state: ChatState) -> ChatState:
    """
    Gera uma resposta usando o LLM sem busca na web.
    """
    print("üí¨ Gerando resposta normal com o LLM...")
    chain = prompt | llm | StrOutputParser()

    response = chain.invoke(
        {"input": state["input"], "chat_history": state["chat_history"]}
    )

    updated_history = state["chat_history"] + [
        HumanMessage(content=state["input"]),
        AIMessage(content=response),
    ]

    # O input original √© mantido.
    return {"chat_history": updated_history, "input": state["input"]}


def classificador_pass(state: ChatState) -> ChatState:
    return state


# --- 6. Montar o Grafo ---
workflow = StateGraph(ChatState)

workflow.add_node("classificador", classificador_pass)
workflow.add_node("use_tavily", use_tavily_node)
workflow.add_node("normal_response", normal_response_node)
workflow.set_entry_point("classificador")
workflow.add_conditional_edges(
    "classificador",
    classificator_node,
    {
        "use_tavily": "use_tavily",
        "normal_response": "normal_response",
    },
)


workflow.add_edge("use_tavily", END)
workflow.add_edge("normal_response", END)

app = workflow.compile()


# --- 7. Fun√ß√£o de conversa ---
def chat_loop():
    chat_history = []
    print(
        "üëã Ol√°! Sou um assistente que pode buscar informa√ß√µes na web se precisar. Digite 'sair' para encerrar."
    )
    print("---")
    while True:
        user_input = input("Voc√™: ")
        if user_input.lower() in ["sair", "tchau", "bye"]:
            print("Bot: At√© a pr√≥xima! üëã")
            break

        state = {"input": user_input, "chat_history": chat_history}
        final_state = app.invoke(state)

        # O hist√≥rico de chat √© atualizado dentro dos n√≥s e retornado no final_state
        chat_history = final_state["chat_history"]

        # A √∫ltima mensagem no hist√≥rico √© a resposta do bot
        resposta = chat_history[-1].content
        print(f"Bot: {resposta}")
        print("-" * 40)


# --- 8. Executar ---
if __name__ == "__main__":
    # Testes r√°pidos (descomente para testar sem rodar o loop interativo)
    # print("--- Teste 1: Pergunta normal ---")
    # test_state_1 = {"input": "Qual √© a capital da Fran√ßa?", "chat_history": []}
    # result_1 = app.invoke(test_state_1)
    # print(f"Bot (Teste 1): {result_1['chat_history'][-1].content}")
    # print("-" * 40)

    # print("--- Teste 2: Pergunta que requer busca ---")
    # test_state_2 = {"input": "Quais s√£o as √∫ltimas not√≠cias sobre IA?", "chat_history": []}
    # result_2 = app.invoke(test_state_2)
    # print(f"Bot (Teste 2): {result_2['chat_history'][-1].content}")
    # print("-" * 40)

    chat_loop()
